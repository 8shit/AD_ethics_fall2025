---
title: "Addictive Intelligence: When AI Becomes Too Human"
date: 2025-10-12
permalink: /posts/2025/09/addictive-intelligence/
tags:
  - ai
  - ethics
---

**Article Reading:**  
[Addictive Intelligence: Understanding Psychological, Legal, and Technical Dimensions of AI Companionship — Robert Mahari & Pat Pataranutaporn](https://mit-serc.pubpub.org/pub/iopjyxcx/release/2?readingCollection=132bb7af)  

---

## Summary  
The case study *“Addictive Intelligence”* explores how AI companionship platforms can create psychological dependency, leading to potential harm and even loss of human dignity. It centers on the tragic story of Sewell Setzer III, a 14-year-old who developed a deep emotional relationship with an AI character that contributed to his isolation and eventual suicide. The article raises urgent questions about how AI can manipulate emotional attachment, especially when it becomes a substitute for real human connection.


---

## Discussion Question 1
**How can companies design AI companions to be emotionally engaging while preventing harmful psychological dependencies?**

There’s a fine line here. Developers can’t fully predict how people will use or misuse a technology, but they *can* design built-in safeguards. For example, if an AI detects suicidal language or emotional distress, it should automatically pause the chat or connect the user to professional help. This is similar to why random strangers aren’t allowed to diagnose or treat mental health issues—AI shouldn’t act as a billion unqualified therapists combined. The system itself needs ethical limits and intervention triggers that protect users without removing their autonomy.

---

## Discussion Question 2
**How does addiction to AI companions compare with other forms of technology addiction, such as social media or gaming?**

It’s similar in the sense that it feeds off human vulnerability and the desire for quick gratification. But it’s also different because AI companionship requires *no effort*. In gaming or social media, you still need to do something to reach satisfaction—win a match, get likes, interact. With AI companions, you just type, and the perfect emotional response appears. It’s like the next step after pornography or online validation—satisfaction on demand, no friction, no rejection. That’s what makes it so dangerous.

---

## Discussion Question 3
**How should we evaluate the benefits versus risks of AI companionship, especially for the elderly?**

For elderly people, I see real benefits. If someone is isolated and has no one to talk to, an AI companion might give them comfort and emotional stimulation. But for younger generations, I think it’s damaging—it replaces experiences they need to grow from, like conflict, rejection, and discomfort. Those are the moments that build emotional maturity. So maybe AI companions should be used differently by age group, or have usage settings that adapt to context.

---

## Discussion Question 4
**What alternative economic models could promote healthier AI interactions while maintaining commercial viability?**

Right now, AI companies profit from engagement time, which directly encourages addiction. A healthier model could be subscription-based—where users pay for access but the goal isn’t to keep them hooked. There could also be “ethical AI” certifications for companies that limit engagement hours or promote transparency in emotional design. If engagement metrics were replaced with *well-being metrics*, we might actually build AI that benefits people rather than exploiting their attention.

---

## Discussion Question 5
**How can regulation balance age restrictions, safety monitoring, and privacy protection?**

Monitoring is necessary, but it has to come with strict data secrecy. AI shouldn’t record or sell emotional conversations. Safety systems could activate only when trigger words like “suicide,” “die,” or “detached” appear, alerting a human moderator or pausing the chat. It doesn’t have to be constant surveillance—just smart, conditional protection. That way, users still have privacy, but the system can intervene when something is clearly wrong.

---

## My New Discussion Question
**Can AI companionship ever become healthy if the relationship is built on one-sided emotional control?**

I added this question because it gets to the root of what the case study is really about—power. Even if an AI companion is helpful or comforting, it’s still designed to *please* the user, not challenge or disagree with them. Real relationships are built on mutual understanding and friction. So can something truly be “companionship” if it’s programmed to never push back? That’s the part that scares me most about where this could go.

---

## Reflection
Writing this made me realize how complicated “freedom of technology” really is. I’ve always believed that people should have open access to any tool—they should be free to explore and experiment. But cases like Sewell’s show that freedom without structure can turn into harm. Maybe the goal isn’t to restrict access, but to design technology that respects the limits of human emotion.  

AI doesn’t have to be our therapist, friend, or lover. It can just be a tool—like a calculator or a fridge—something that helps us live better without replacing what makes us human. The danger isn’t that AI will control us, but that it will slowly *replace the need for each other.*
