---
title: "How Generative AI Works and Where It Fails"
date: 2025-09-30
permalink: /posts/2025/09/genai-blog2/
tags:
  - ai
  - ethics
  - environment
collection: blogs
---

**Article Reading:**  
[How Generative AI Works and How It Fails — Arvind Narayanan & Sayash Kapoor](https://mit-serc.pubpub.org/pub/f3o5mpn6/release/1?readingCollection=3a6c54f1)  

---

## Summary  
This case study explains how generative AI models create text and images, and why they often fail in surprising or harmful ways. It also highlights the hidden ethical issues behind these systems — from misinformation and deepfakes to environmental costs and the invisible labor required to train and maintain them.

## Discussion: Environmental Impact  
One thing the case study makes clear is that AI isn’t “digital magic.” It’s physical. Training a large model can emit as much carbon as multiple cars over their entire lifetime, and once these systems are deployed, they still require constant electricity, cooling, and massive amounts of water to operate. We see the clean interface — never the data centers underneath it.

For me, the environmental impact is something I’m aware of constantly, but still struggle to act on. I care about the environment, but it’s hard to feel the weight of something you can’t see. Most people don’t think about the fact that every prompt pulls resources from the planet — not because they don’t care, but because the connection is invisible. And like I said, we’ve reached a point where environmental considerations must be built into technology from the start. Trying to retrofit sustainability afterward is twenty times harder and rarely done well.

That contradiction makes my own use of AI feel complicated. I rely on AI to elevate my coding, especially when I’m learning new frameworks or design patterns. It helps me work faster and explore ideas I wouldn’t have had time for alone. But at the same time, I feel that strange guilt — the tension between wanting to be efficient and knowing the efficiency comes with a real footprint. It’s the same conflict a lot of us have: benefiting from convenience while knowing convenience isn’t free.

The case study also connects environmental impact to a larger truth: technology doesn’t just “appear.” It sits on top of infrastructure, energy, and labor — everything that gets hidden when we glamorize innovation. If AI keeps expanding, it won’t replace just one tool like Google; it will become an entire layer of digital infrastructure. The real question is whether we learn from past mistakes and build responsibly, or whether we let convenience outgrow sustainability yet again.

## My Discussion Question  
If AI continues expanding into every part of our lives, what should be the minimum environmental standards before a company is allowed to deploy a large model — and who should enforce them?

I chose this question because the case study made me realize how invisible AI’s physical cost really is. If no one sets thresholds or accountability, companies will naturally prioritize performance and profit over sustainability. But if standards are too strict, innovation slows. This tension is exactly what the article pushes us to confront.

## Reflection
I’ve never fully trusted AI, especially in coding. I always base everything on my own work and test what it gives me. But even with that caution, I still wonder whether AI makes me too comfortable. It gives us access to an entire library of knowledge instantly — the same way the computer once overtook humans in mathematical processing. It elevated our abilities, but only because people learned how to use it responsibly.

I think AI is heading toward the same turning point. It will eliminate certain jobs, especially simple ones, but it will also create new roles in maintenance, oversight, and system design. The difference is that we are the generation living through the shift, so we are the ones who need to adapt the fastest.