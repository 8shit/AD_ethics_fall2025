---
title: "AI’s Regimes of Representation: When Technology Forgets Who We Are"
date: 2025-10-24
permalink: /posts/2025/10/bias-blog1/
tags:
  - ai
  - bias
  - ethics
  - representation
collection: blogs
---

**Article Reading:**  
[AI’s Regimes of Representation: A Community-Centered Study of Text-to-Image Models in South Asia — Rida Qadri, Renee Shelby, Cynthia L. Bennett, and Remi Denton](https://mit-serc.pubpub.org/pub/bfw5tscj/release/3?readingCollection=65a1a268)

---

## Bias Blog 1: Right to Fair Representation  

Reading *AI’s Regimes of Representation: A Community-Centered Study of Text-to-Image Models in South Asia* made me think about how technology reflects the world we live in and how often it gets that reflection wrong. The study looks at how generative AI, the kind that turns text into images, struggles to represent non-Western cultures accurately. It’s not just about errors in the data but about deeper issues of perspective and power.  

When I think about cultural representation, I think about the difference between being seen and being understood. Growing up Lebanese, I’ve seen how often Arab identity is simplified or misread. Sometimes it’s small things, like Arabic text getting flagged online as suspicious. Other times it’s how entire regions are portrayed through one lens, violent, chaotic, or poor. These ideas get repeated so often that they start to feel like the only image people know. And now, when AI learns from the same internet that built those stereotypes, the bias becomes built into the technology itself.  

What stood out to me in the study was how the researchers didn’t just test the models, they listened to people. They asked South Asian participants how the images made them feel and what felt wrong or incomplete. That kind of feedback matters because you can’t always measure fairness through numbers. A dataset might say a model performs well, but that doesn’t mean the images reflect people’s actual lives. Representation isn’t just about accuracy, it’s about connection, about whether someone can look at what the AI creates and say, “yes, that feels real.”  

Small, community-based evaluations like this one seem more valuable than the usual technical benchmarks. They add the human context that algorithms miss. A machine might know what a mosque looks like but not what it means to the people who built it, or how its design connects to a sense of identity and history. Without that context, AI will always create images that look right on the surface but feel empty underneath.  

The study also made me think about inclusion. I do think AI can become more inclusive, but it’s not just about adding more data from different countries. Inclusion has to start with who’s in the room. Most of the time, the people who build these systems don’t come from the places that end up being misrepresented. If communities had a say in how their culture appears in training data, we’d get closer to a system that reflects people instead of labeling them. It’s not about perfect accuracy, but about giving people control over their own image.  

Developers could learn a lot from that idea. They could work with artists, historians, and local voices to review how their culture is shown. They could also be transparent about where the data comes from. Most importantly, they need to understand that bias isn’t a technical glitch. It’s social, historical, and built into the way we see each other. You can’t just code that away, you have to talk about it.  

Another thing I found interesting is the question of whether culture can really be encoded into an algorithm. I don’t think it can, at least not completely. Culture changes too often for that. But models could be built to learn and adapt over time. Maybe it’s not about teaching AI to get it perfectly right, but about giving it space to grow, just like people do.  

Looking back at the history of media and technology, the same pattern repeats. Every tool ends up reflecting whoever built it. Photography, film, social media, all of them carried the bias of their creators. The problem isn’t the technology itself, but who gets to define what’s normal or true. If AI is going to be different, it has to start with that awareness. Representation isn’t neutral, it’s power.  

It makes me wonder what AI would look like if the people it’s misunderstood the most, Arabs, South Asians, Africans, Indigenous communities, were the ones designing the datasets and deciding what counts as accurate. Maybe the whole idea of fairness would look different if those voices were centered from the start.  

This case reminded me that fairness in AI isn’t just a technical goal, it’s about dignity. When we’re represented unfairly, it shapes how others see us and how we see ourselves. I want AI that doesn’t just show what we look like but understands what those images mean. Maybe that’s what responsible technology really is, not something that knows everything, but something that’s built with care, curiosity, and respect.  
