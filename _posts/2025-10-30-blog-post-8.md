---
title: "Understanding Potential Sources of Harm throughout the Machine Learning Life Cycle"
date: 2025-10-30
permalink: /posts/2025/10/bias-blog2/
tags:
  - ai
  - ethics
  - machine-learning
  - bias
collection: blogs
---

**Article Reading:**  
[Understanding Potential Sources of Harm throughout the Machine Learning Life Cycle — Harini Suresh & John Guttag](https://mit-serc.pubpub.org/pub/potential-sources-of-harm-throughout-the-machine-learning-life-cycle/release/2)

---
## Summary

This case study explains how harm in machine learning does not come from a single mistake, but can emerge at every stage of the model’s life cycle, from data collection and measurement to deployment and real-world use. It shows that even technically accurate systems can still produce unfair and harmful outcomes because of human design choices.

## Discussion

Machine learning tools are already deeply embedded in my daily life, especially through platforms like ChatGPT, which I mostly use for brainstorming, organizing ideas, and understanding difficult concepts. Because it feels helpful and intuitive, it’s easy to forget that these systems are shaped by human assumptions at every step. One of the biggest ideas that stood out to me was measurement bias, where complex human traits get reduced to simple proxies that don’t truly represent reality. The article’s example of using arrests as a proxy for criminal behavior really stuck with me because it shows how easily data can misrepresent the truth. I see the same thing happen online when platforms try to label “toxicity” or “safety” with rigid categories that cannot fully capture cultural context.

Representation bias also felt very personal to me, especially when thinking about hiring algorithms. I strongly believe that resume-screening models do not give people a fair chance when they rely heavily on keyword matching. In my case, having Arabic listed as one of my languages could easily be misinterpreted by an automated system. Arabic is not a language most people randomly pick up; it often signals heritage. An algorithm does not understand that cultural layer. It only sees deviation from the “expected” pattern. That means someone can be filtered out before a human even looks at their qualifications, which makes bias feel quiet but extremely powerful.

Deployment bias was another idea that changed how I see AI in the real world. A model might be built for one purpose and then quietly repurposed for something far more serious, like risk assessments influencing legal or financial decisions. What worries me most is how easily people trust these systems once they are labeled as “objective” or “data-driven.” The model itself cannot set boundaries on how it is used. Humans do. That means harm doesn’t come from code alone, but from how much authority we hand over to automated systems without questioning their limits.

When it comes to responsibility, I see it as shared. Developers are responsible for building safeguards into systems from the start. Corporations are responsible because they often choose profit and speed over long-term harm prevention. Users also carry responsibility when they rely on these tools to judge others in unfair ways. For me, the biggest risks right now are over-reliance and privacy. The more convenient these systems become, the more people trust them without thinking critically. At the same time, enormous amounts of personal data are being collected, stored, and reused in ways most users never fully understand.


## New Discussion Question

New Question:
If machine learning systems will always contain human bias at every stage of their life cycle, should the goal still be “perfect fairness,” or should the focus shift toward limiting harm and increasing transparency instead?

I included this question because the case study made me realize that complete fairness may be impossible when human judgment is embedded into every phase of these systems. Rather than chasing a perfect model, it may be more realistic and more ethical to prioritize transparency, accountability, and harm reduction. This question gets at the tension between ideal solutions and practical responsibility.

## Reflection

This exercise made me rethink how casually I interact with machine learning systems in my own life. I always considered myself hopeful but cautious about AI, and this reading strengthened both sides of that feeling. It showed me that harm is not just the result of “bad data,” but of many small human decisions stacked on top of each other. What felt most unsettling is how invisible most of this process is to everyday users. I still believe these tools can be powerful and beneficial, especially for learning and creativity, but I now feel a stronger responsibility to question how they work and how much trust they deserve. This case study pushed me to stop seeing machine learning as neutral technology and start seeing it as a system of power, assumptions, and consequences that directly affects real people.
