---
title: "What Is Work Worth? Generative AI, Labor, and the Future of Human Effort"
date: 2025-12-2
permalink: /posts/2025/09/blog-5/
tags:
  - ai
  - ethics
  - labor
collection: blogs
---

**Article Reading:**  
[What is Work Worth? Exploring What Generative AI Means for Workers’ Lives and Labor | Keynote Event](https://www.youtube.com/watch?v=5DDwfRmQhV0&list=PLYrf5LyVCF1Nk28nRu8lcCIxtAH3iVGDj&index=1)  

---

## Summary  
The keynote “What Is Work Worth?” features Data & Society’s Labor Futures Program Director Aiha Nguyen and Dr. Julián Posada, a Yale researcher who studies AI labor and digital inequalities. Together, they show how generative AI depends on global networks of human labor — especially data annotators and content moderators — and why AI must be understood not only as technology but as a labor system shaped by extractivism, inequality, and cultural standardization.

## Discussion:  
Watching this keynote reminded me that most conversations about AI focus on automation, productivity, or fear of job loss — but rarely on the workers who make AI possible. Posada’s emphasis on extractivist technology made me rethink the entire structure behind the tools we casually rely on. He describes AI not as an independent system but as one built on thousands of invisible workers across places like Venezuela, Kenya, and India. Companies strategically rely on regions in economic crisis, where people have enough connectivity to work but few alternatives. This pattern reflects broader global inequalities that I’ve seen in other fields, where developing economies become the labor engines for wealthier nations.

What connected most to my own experience was his point about cultural erasure. When workers label hate speech or generate training data using U.S.-centric definitions, their own realities get overwritten. That immediately resonated with me because, growing up Lebanese, I know what it’s like to have your cultural context misread or flattened by systems built elsewhere. It’s the same dynamic I see with AI: the model doesn’t learn the diversity of the world — it learns the worldview of whoever built the dataset.

The keynote also challenged how I think about the future of work. I’ve said before that AI doesn’t just destroy jobs — it rearranges them — and this talk reinforced that, but with nuance I hadn’t fully considered. Yes, new roles will emerge around data centers, oversight, and system maintenance, but who gets those roles, and who gets left with the precarious ones? The global supply chain of digital labor makes it clear that some countries benefit more than others from “innovation.” As someone entering the workforce during this transition, it feels like we are caught between two realities: AI makes our work faster and more powerful, but it also raises new ethical responsibilities about how that power is built.

I also saw myself in the tension Posada described: people use AI even when they know its harms because the infrastructure of modern life pushes them toward it. I use AI to speed up coding, documentation, and learning new frameworks, and it genuinely helps. But the more convenient it becomes, the easier it is to forget the labor behind it. This guilt mirrors the same feelings people have about fast fashion or global supply chains — the product is useful, but the cost is hidden.

The keynote matters because it pushes us to shift the conversation from “Will AI replace us?” to “What kinds of labor does AI rely on, and who pays the price?” That reframing is essential if we want AI development to be ethical rather than extractive.

## My Question  
If AI relies on global networks of invisible labor, should companies be legally required to disclose the human work behind their models — including wages, working conditions, and cultural impacts — in the same way companies now disclose environmental or manufacturing supply chains?

Why I chose it:
The talk showed that invisibility is not an accident — it is the system. Transparency could force companies to treat these workers like real contributors, not disposable parts of a pipeline. Without visibility, there’s no accountability.

## Reflection
Posada’s keynote pushed me to look beyond the capabilities of AI and examine the structure that makes those capabilities possible. It made me realize that the value of work is not just measured by output — it’s measured by dignity, agency, and recognition. As someone who actively uses AI, I felt challenged to think about my own position in this ecosystem: benefitting from a tool that others have paid a much higher price to build.

It also made me think about long-term generational shifts. Like the introduction of computers, AI will probably disrupt the job market before it stabilizes. But that stabilization will only be fair if we consciously design a system that respects the people behind the automation — both those who supervise the models in the U.S. and those doing the less visible work across the globe.

Ultimately, the keynote reminded me that the future of AI isn’t just about innovation — it’s about responsibility. If we want AI to elevate human life, then the humans who build it, train it, and sustain it must be treated with value, transparency, and respect.