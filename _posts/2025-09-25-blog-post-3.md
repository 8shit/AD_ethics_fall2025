---
title: "The Right to Be an Exception to a Data-Driven Rule"
date: 2025-09-25
permalink: /posts/2025/09/right-to-be-an-exception/
tags:
  - ai-ethics
  - governance
  - uncertainty
  - fairness
---

**Article Reading:**  
[The Right to Be an Exception to a Data-Driven Rule — Sarah H. Cen & Manish Raghavan](https://mit-serc.pubpub.org/pub/right-to-be-exception/release/2)  

---

## Summary  
This case study argues that people deserve the right to be treated as exceptions when algorithms make important decisions about their lives. Even if a model performs well on average, it can still fail the individual standing in front of it , and the burden should fall on the system to prove it is safe and appropriate, not on people to justify why they shouldn’t be harmed.

## Discussion  
Algorithms behave as if the world is clean and predictable: collect features, process them, output a decision. That works for data, but people aren’t that tidy. A model can be “accurate” and still completely miss the reality of someone’s circumstances. I kept thinking about how my own trading models look brilliant most of the time, until they hit edge conditions I never planned for. Being an exception doesn’t mean something is wrong with you , it means the system wasn’t built with you in mind.

A data-driven rule assumes patterns apply equally to everyone. But in real life, the line between “pattern” and “misrepresentation” gets blurry. I think about job applications I’ve submitted: on paper I’m categorized as a white American, but the moment Arabic shows up as one of my languages, I wonder whether the system flags that as something unusual. It’s a small thing, but it reminds me that even “neutral” data points can trigger assumptions that have nothing to do with who I actually am. Algorithms don’t understand heritage, nuance, or context; they just see deviations from the statistical norm.

Another difference between human and algorithmic decision-making is scale. A human might make a mistake once. An algorithm makes the exact same mistake thousands of times with the confidence of being “95% accurate.” But that 5% , the people the model misclassifies , matters when the stakes are things like housing, healthcare, loans, freedom, or employment. Accuracy can easily hide harm.

Individualization feels like the obvious fix: treat people based on their real circumstances, not on averages. It sounds fair, and it helps people understand what they can actually change. But it also introduces new challenges , more data collection, more complexity, and more opportunities for people to game the system. The case study also raised an uncomfortable question for me: who pays for fairness?

Personally, I don’t believe individuals should ever bear the cost of algorithmic mistakes. If an institution chooses to use algorithmic screening, they should fund the safety checks and oversight. And if the data isn’t comprehensive enough to understand the full range of human situations, then algorithmic decision-making for high-stakes situations should either be banned or supplemented with mandatory human review. It might be less efficient, but efficiency is not an excuse for harming people. Morally, fairness comes first.

## My Discussion Question
If algorithms are known to fail at the edges, should every high-stakes decision require an automatic human override option , and if so, who decides what counts as “high-stakes”?

I chose this question because the article made me think about how uncertain we really are when we hand authority to models. If we don’t define “high-stakes,” organizations will choose whatever is cheapest. But if we define it too broadly, nothing gets automated at all. The challenge lies exactly in that balance.

## Reflection  
What stayed with me is how fragile “accuracy” becomes once you look at individuals instead of averages. Models don’t break in the middle; they break at the margins , on exceptions, on outliers, on people whose lives don’t match the pattern. The right to be treated as an exception isn’t about resisting technology; it’s about insisting that systems prove they can handle uncertainty before affecting someone’s future. If algorithms are going to make decisions about us, they should have to earn that trust , not assume it.