---
title: "The Right to Be an Exception to a Data-Driven Rule"
date: 2025-09-25
permalink: /posts/2025/09/right-to-be-an-exception/
tags:
  - ai-ethics
  - governance
  - uncertainty
  - fairness
---

**Article Reading:**  
[The Right to Be an Exception to a Data-Driven Rule — Sarah H. Cen & Manish Raghavan](https://mit-serc.pubpub.org/pub/right-to-be-exception/release/2)  

---

## Summary

This case study argues that people should have the **right to be an exception** when algorithms are making big decisions about their lives—like getting a job, a loan, or parole. The idea is that we shouldn’t just assume a model works for everyone; the people using it should have to prove it fits the person in front of them, taking into account individualization, uncertainty, and the possible harm.

---

## Discussion

**1) What is a data-driven rule, and what does it mean to be a data-driven exception? Is an exception the same as an error?**  
A data-driven rule is basically the model’s recipe: you put in features, it spits out a decision. An exception is someone the recipe doesn’t fit well, even if the model is “accurate” overall. That’s not the same thing as an error—sometimes the output is technically right, but it still wasn’t fair or reasonable to apply that rule to this person.  
*For me, this feels like trading models: they can look great on paper, but in certain situations they just don’t apply. If you’re in one of those situations, you’re the exception, even if the trade accidentally works out.*

---

**2) In addition to those listed above, what other factors differentiate data-driven decisions from human ones?**  
- Algorithms repeat the **same mistakes** over and over, while human mistakes are usually more random.  
- Once a model is adopted, organizations tend to **stick with it** and treat it as the standard.  
- Humans can explain their choices; models can’t really do that.  
- Algorithms are fast and consistent, which is good until they’re wrong—then they’re wrong at scale.

---

**3) Beyond what is discussed above, what are some of the benefits and downsides of individualization?**  
**Upsides:** It feels fairer, it looks at details that actually apply to me, and it gives people a clearer idea of what they can change for next time.  
**Downsides:** It can mean giving up too much personal data, it encourages people to game the system (“just get this credential to pass the filter”), and it makes models harder to keep track of. Plus, adding more detail isn’t always better—sometimes it just causes overfitting.

---

**4) Why is uncertainty so critical to the right to be an exception? Can accuracy alone justify high-stakes use?**  
Uncertainty is the key issue. Some uncertainty comes from not having enough data (epistemic), and some is just randomness we can’t get rid of (aleatoric). Either way, if the harm from a wrong call is big, accuracy alone doesn’t cut it. Saying a model is “95% accurate” still means 1 in 20 people might be harmed—and that’s too high when we’re talking about freedom, housing, or healthcare.

---

## New question

**Question:** If we really enforce a right to be an exception, who ends up paying for the extra work it takes—organizations (more staff and reviews), the public (slower services), or individuals (appeals and time)?  

**Why I chose it:** The framework sounds good, but fairness takes resources. I think it’s worth asking who should take on those costs, because that’s what decides whether this idea can actually work in practice.

---

## Reflection

This case study made a lot of sense to me. In modeling, it’s almost always the edge cases—the “exceptions”—that break things, not the average case. Society isn’t any different. We can’t just brush off the people who don’t fit the model when the harm is that serious. The main point that stuck with me is the **burden of proof** should flip: it shouldn’t be on people to prove they deserve fair treatment, it should be on the decision maker to show the model is actually safe to use on them. To me, that feels both fair and practical.

---
