---
title: "Censorship of Misinformation and Freedom of Speech on Social Media"
date: 2025-11-10
permalink: /posts/2025/11/info-blog1/
tags:
  - social-media
  - ethics
  - misinformation
  - free-speech
collection: blogs
---

**Article Reading:**  
[Censorship of Misinformation and Freedom of Speech on Social Media â€” Kevin A. I. Mills](https://mit-serc.pubpub.org/pub/lkf63cu5/release/1)

---

## Summary

This case study examines whether censoring misinformation on social media protects society or weakens democratic trust. Mills argues that the real crisis is not false information itself, but the breakdown of public trust in institutions, media, and expertise.

## Discussion

Misinformation feels personal to me because I have seen how easily people I care about can fall for it. My parents have both been targeted by online scams before, and even though none were created directly by AI, it showed me how vulnerable people can be when information feels urgent or convincing. Because of that, I double check almost everything when I research something online. I do not trust information just because it looks polished or popular. That habit came from watching how quickly false claims can spread when people are scared or unsure.

When it comes to censorship, I am generally against it, but I support strong moderation. I think information should stay accessible with as little withholding as possible, but platforms still have a responsibility to protect users. Graphic content, manipulated images, and harmful material targeting minors should absolutely be moderated or flagged. For false information, I believe warnings and context are more powerful than outright removal. If a post is simply deleted, it often strengthens the belief that someone is being silenced. That just feeds into the distrust Mills talks about in the reading.

For news, I mostly rely on news apps and WhatsApp broadcast groups from back home because they are fast and usually focused on local realities. Sometimes I use Gemini just to summarize headlines so I can compare multiple sources quickly. Still, I am careful. I do not fully trust any single source on its own. Trust, for me, comes from seeing the same facts supported across different platforms and from sources that admit mistakes when they make them.

Algorithms definitely push people into echo chambers. I see it clearly on platforms like TikTok and YouTube. The moment you interact with one topic, you are pulled deeper into a constant stream of similar content. It becomes addictive very quickly. What troubles me the most is how this same mechanism is used to capitalize on fear, insecurity, and even politics. Once you fall into one narrative, it becomes harder to escape it, even when conflicting evidence exists.

Responsibility for misinformation is shared. Users are responsible for not blindly sharing content and for learning basic fact checking. Companies are responsible for how neutral they pretend to be while their algorithms clearly amplify extreme content because it makes money. Governments also use misinformation for their own benefit when it suits them. And algorithms ultimately reflect the priorities of the developers and corporations that design them. Profit keeps winning over public good, and that is the core of the problem.

Emotionally, misinformation exhausts me. It feels like there is always another lie to correct, another rumor spreading, another distorted version of reality taking over. But I still push back whenever I can. Sometimes that means sharing the correct information. Sometimes it just means stopping a rumor before it spreads further in my own circles. I do not think one person can solve the problem, but I do think silence always makes it worse.

## New Discussion Question

If you were given full control over moderation on a major social media platform, how would you balance free speech with public safety without becoming the very authority people distrust?

I included this question because moderation sounds simple in theory but becomes extremely complex in practice. It forces us to confront our own values, fears, and limits instead of pretending that an app or an algorithm can solve it for us.

## Reflection

This case study helped me realize that the fight over misinformation is really a fight over trust. Technology did not create dishonesty, but it made it faster, louder, and harder to escape. I do not believe heavy censorship will fix that crisis. At the same time, I also do not believe in complete freedom without responsibility. What we need is not less speech, but better information systems and more honest public accountability.

I walked away from this reading feeling tired but also more aware. Exhausted because the problem is so deep and connected to power, money, and politics. Hopeful because I still believe people can learn to question what they see. In the end, I think democracy does not collapse because people speak too freely. It collapses when people stop caring whether what they hear is true.



